{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Search using tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_shark = \"Let's face it: Baby Shark is an undeniable force. James invites his guests, Sophie Turner, playing the role of Mommy Shark, and Josh Groban, taking on the role of Daddy Shark, for the definitive performance of this global phenomenon. More Late Late Show: \\\n",
    "Subscribe: http://bit.ly/CordenYouTube \\\n",
    "Watch Full Episodes: http://bit.ly/1ENyPw4 \\\n",
    "Facebook: http://on.fb.me/19PIHLC \\\n",
    "Twitter: http://bit.ly/1Iv0q6k \\\n",
    "Instagram: http://bit.ly/latelategram \\\n",
    "Watch The Late Late Show with James Corden weeknights at 12:35 AM ET/11:35 PM CT. Only on CBS. \\\n",
    "Get new episodes of shows you love across devices the next day, stream live TV, and watch full seasons of CBS fan favorites anytime, anywhere with CBS All Access. Try it free! http://bit.ly/1OQA29B \\\n",
    "---\\\n",
    "Each week night, THE LATE LATE SHOW with JAMES CORDEN throws the ultimate late night after party with a mix of celebrity guests, edgy musical acts, games and sketches. Corden differentiates his show by offering viewers a peek behind-the-scenes into the green room, bringing all of his guests out at once and lending his musical and acting talents to various sketches. Additionally, bandleader Reggie Watts and the house band provide original, improvised music throughout the show. Since Corden took the reigns as host in March 2015, he has quickly become known for generating buzzworthy viral videos, such as Carpool Karaoke.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data= tokenizer.tokenize(baby_shark.lower())\n",
    "#https?\\:\\/\\/[A-Z, a-z, \\., \\/, 0-9]+\n",
    "def tokenize2(text):\n",
    "    text= text.lower()\n",
    "    return re.findall(r'\\w+', text)\n",
    "# data=tokenize2(baby_shark)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_to_text={}\n",
    "title_to_index={}\n",
    "\n",
    "\n",
    "with open('./data/medium/medium-data-small.json') as f:\n",
    "    data = json.load(f)\n",
    "i=0\n",
    "for medium in data:\n",
    "    title_to_index[medium[\"title\"]]=i\n",
    "    title_to_text[medium[\"title\"]] = tokenize2(medium[\"text\"])\n",
    "    i+=1\n",
    "\n",
    "    \n",
    "\n",
    "def getLink(query):    \n",
    "    if(query == \"hi\"):\n",
    "        return [data[0][\"link\"],data[1][\"link\"],data[2][\"link\"],data[3][\"link\"],data[4][\"link\"]]\n",
    "    else:\n",
    "        return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(title_to_index[data[2][\"title\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine', 'learning', 'code', 'can', 'be', 'notoriously', 'difficult', 'to', 'debug', 'with', 'bugs', 'that', 'are', 'expensive', 'to', 'chase', 'even', 'for', 'simple', 'feedforward', 'neural', 'networks', 'you', 'often', 'have', 'to', 'make', 'several', 'decisions', 'around', 'network', 'architecture', 'weight', 'initialization', 'and', 'network', 'optimization', 'all', 'of', 'which', 'can', 'lead', 'to', 'insidious', 'bugs', 'in', 'your', 'machine', 'learning', 'code', 'as', 'chase', 'roberts', 'wrote', 'in', 'an', 'excellent', 'piece', 'on', 'how', 'to', 'unit', 'test', 'machine', 'learning', 'code', 'his', 'frustrations', 'stemmed', 'from', 'common', 'traps', 'like', 'so', 'what', 'is', 'to', 'be', 'done', 'about', 'it', 'this', 'article', 'will', 'provide', 'a', 'framework', 'to', 'help', 'you', 'debug', 'your', 'neural', 'networks', 'feel', 'free', 'to', 'skip', 'to', 'a', 'particular', 'section', 'or', 'read', 'through', 'below', 'please', 'note', 'we', 'do', 'not', 'cover', 'data', 'preprocessing', 'or', 'specific', 'model', 'algorithm', 'selection', 'there', 'are', 'plenty', 'of', 'great', 'resources', 'for', 'those', 'topics', 'online', 'for', 'example', 'check', 'out', 'choosing', 'the', 'right', 'machine', 'learning', 'algorithm', 'a', 'neural', 'network', 'that', 'has', 'a', 'complex', 'architecture', 'with', 'regularization', 'and', 'a', 'learning', 'rate', 'scheduler', 'will', 'be', 'harder', 'to', 'debug', 'than', 'a', 'simple', 'network', 'we', 're', 'kind', 'of', 'cheating', 'with', 'this', 'first', 'point', 'since', 'it', 's', 'not', 'really', 'related', 'to', 'debugging', 'a', 'network', 'you', 've', 'already', 'built', 'but', 'it', 's', 'still', 'an', 'important', 'recommendation', 'start', 'simple', 'by', 'to', 'start', 'off', 'build', 'a', 'small', 'network', 'with', 'a', 'single', 'hidden', 'layer', 'and', 'verify', 'that', 'everything', 'is', 'working', 'correctly', 'then', 'gradually', 'add', 'model', 'complexity', 'while', 'checking', 'that', 'each', 'aspect', 'of', 'your', 'model', 's', 'structure', 'additional', 'layer', 'parameter', 'etc', 'works', 'before', 'moving', 'on', 'as', 'a', 'quick', 'sanity', 'check', 'you', 'can', 'use', 'one', 'or', 'two', 'training', 'data', 'points', 'to', 'confirm', 'whether', 'your', 'model', 'is', 'able', 'to', 'overfit', 'the', 'neural', 'network', 'should', 'immediately', 'overfit', 'with', 'a', 'training', 'accuracy', 'of', '100', 'and', 'a', 'validation', 'accuracy', 'that', 's', 'commensurate', 'to', 'your', 'model', 'randomly', 'guessing', 'if', 'your', 'model', 'is', 'unable', 'to', 'overfit', 'just', 'those', 'data', 'points', 'then', 'either', 'it', 's', 'too', 'small', 'or', 'there', 'is', 'a', 'bug', 'even', 'when', 'you', 've', 'verified', 'that', 'your', 'model', 'is', 'working', 'try', 'train', 'for', 'a', 'single', 'or', 'a', 'few', 'epochs', 'before', 'progressing', 'your', 'model', 's', 'loss', 'is', 'the', 'primary', 'way', 'to', 'evaluate', 'your', 'model', 's', 'performance', 'and', 'it', 's', 'what', 'the', 'model', 'is', 'evaluating', 'to', 'set', 'important', 'parameters', 'so', 'you', 'want', 'to', 'make', 'sure', 'that', 'it', 's', 'also', 'important', 'to', 'pay', 'attention', 'to', 'your', 'initial', 'loss', 'check', 'to', 'see', 'if', 'that', 'initial', 'loss', 'is', 'close', 'to', 'your', 'expected', 'loss', 'if', 'your', 'model', 'started', 'by', 'guessing', 'randomly', 'in', 'the', 'stanford', 'cs231n', 'coursework', 'andrej', 'karpathy', 'suggests', 'the', 'following', 'for', 'a', 'binary', 'example', 'you', 'would', 'simply', 'do', 'a', 'similar', 'calculation', 'for', 'each', 'of', 'your', 'classes', 'let', 's', 'say', 'your', 'data', 'is', '20', '0', 's', 'and', '80', '1', 's', 'your', 'expected', 'initial', 'loss', 'would', 'work', 'out', 'to', '0', '2ln', '0', '5', '0', '8ln', '0', '5', '0', '693147', 'if', 'your', 'initial', 'loss', 'is', 'much', 'bigger', 'than', '1', 'it', 'could', 'indicate', 'that', 'your', 'neural', 'network', 'weights', 'are', 'not', 'balanced', 'properly', 'i', 'e', 'your', 'initialization', 'was', 'poor', 'or', 'your', 'data', 'is', 'not', 'normalized', 'to', 'debug', 'a', 'neural', 'network', 'it', 'can', 'often', 'be', 'useful', 'to', 'understand', 'the', 'dynamics', 'inside', 'a', 'neural', 'network', 'and', 'the', 'role', 'played', 'by', 'the', 'individual', 'intermediate', 'layers', 'and', 'how', 'the', 'layers', 'are', 'connected', 'you', 'may', 'be', 'running', 'into', 'errors', 'around', 'if', 'your', 'gradient', 'values', 'are', 'zero', 'it', 'could', 'mean', 'that', 'the', 'learning', 'rate', 'might', 'be', 'too', 'small', 'in', 'the', 'optimizer', 'or', 'that', 'you', 're', 'encountering', 'error', '1', 'above', 'with', 'incorrect', 'expressions', 'for', 'the', 'gradient', 'updates', 'aside', 'from', 'looking', 'at', 'the', 'absolute', 'values', 'of', 'the', 'gradient', 'updates', 'make', 'sure', 'to', 'monitor', 'the', 'magnitudes', 'of', 'activations', 'weights', 'and', 'updates', 'of', 'each', 'layer', 'match', 'for', 'example', 'the', 'magnitude', 'of', 'the', 'updates', 'to', 'the', 'parameters', 'weights', 'and', 'biases', 'should', 'be', '1', 'e3', 'there', 'is', 'a', 'phenomenon', 'called', 'the', 'dying', 'relu', 'or', 'vanishing', 'gradient', 'problem', 'where', 'the', 'relu', 'neurons', 'will', 'output', 'a', 'zero', 'after', 'learning', 'a', 'large', 'negative', 'bias', 'term', 'for', 'its', 'weights', 'those', 'neurons', 'will', 'never', 'activate', 'on', 'any', 'datapoint', 'again', 'you', 'can', 'use', 'gradient', 'checking', 'to', 'check', 'for', 'these', 'errors', 'by', 'approximating', 'the', 'gradient', 'using', 'a', 'numerical', 'approach', 'if', 'it', 'is', 'close', 'to', 'the', 'calculated', 'gradients', 'then', 'backpropagation', 'was', 'implemented', 'correctly', 'to', 'implement', 'gradient', 'checking', 'check', 'out', 'these', 'great', 'resources', 'from', 'cs231', 'here', 'and', 'here', 'and', 'andrew', 'ng', 's', 'specific', 'lesson', 'on', 'the', 'topic', 'faizan', 'shaikh', 'writes', 'about', 'the', 'three', 'primary', 'methods', 'of', 'visualizing', 'your', 'neural', 'network', 'there', 'are', 'many', 'useful', 'tools', 'for', 'visualizing', 'individual', 'layers', 'activations', 'and', 'connections', 'such', 'as', 'conx', 'and', 'tensorboard', 'neural', 'networks', 'have', 'large', 'numbers', 'of', 'parameters', 'that', 'interact', 'with', 'each', 'other', 'making', 'optimization', 'hard', 'please', 'note', 'this', 'is', 'an', 'area', 'of', 'active', 'research', 'so', 'the', 'suggestions', 'below', 'are', 'simply', 'starting', 'points', 'to', 'audit', 'this', 'you', 'should', 'turn', 'off', 'regularization', 'and', 'check', 'your', 'data', 'loss', 'gradient', 'independently', 'it', 's', 'easy', 'to', 'overlook', 'the', 'importance', 'of', 'documenting', 'your', 'experiments', 'until', 'you', 'forget', 'which', 'learning', 'rate', 'or', 'class', 'weights', 'you', 'used', 'with', 'better', 'tracking', 'you', 'can', 'easily', 'review', 'and', 'reproduce', 'previous', 'experiments', 'to', 'reduce', 'duplicating', 'work', 'aka', 'running', 'into', 'the', 'same', 'errors', 'however', 'manually', 'documenting', 'information', 'can', 'be', 'difficult', 'to', 'do', 'and', 'scale', 'for', 'multiple', 'experiments', 'tools', 'like', 'comet', 'ml', 'can', 'help', 'automatically', 'track', 'datasets', 'code', 'changes', 'experimentation', 'history', 'and', 'production', 'models', 'this', 'includes', 'key', 'pieces', 'of', 'information', 'about', 'your', 'model', 'like', 'hyperparameters', 'model', 'performance', 'metrics', 'and', 'environment', 'details', 'your', 'neural', 'network', 'can', 'be', 'very', 'sensitive', 'to', 'slight', 'changes', 'in', 'both', 'data', 'parameters', 'and', 'even', 'package', 'versions', 'leading', 'to', 'drops', 'in', 'model', 'performance', 'that', 'can', 'build', 'up', 'tracking', 'your', 'work', 'is', 'the', 'first', 'step', 'you', 'can', 'take', 'to', 'begin', 'standardizing', 'your', 'environment', 'and', 'modeling', 'workflow', 'we', 'hope', 'this', 'post', 'serves', 'a', 'solid', 'starting', 'point', 'for', 'debugging', 'your', 'neural', 'network', 'to', 'summarize', 'the', 'highlights', 'you', 'should', 'product', 'lead', 'comet', 'ml', 'where', 'we', 're', 'building', 'the', 'github', 'for', 'machine', 'learning', 'sharing', 'concepts', 'ideas', 'and', 'codes']\n"
     ]
    }
   ],
   "source": [
    "print(title_to_text[data[2][\"title\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Mobryan/Desktop/CS4300/assignment5/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9592c7d62dd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtfidf_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdoc_by_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mbaby_shark_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaby_shark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mindex_to_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4300/assignment5/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \"\"\"\n\u001b[0;32m-> 1332\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1333\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4300/assignment5/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 824\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS4300/assignment5/venv/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    772\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "n_feats = 5000\n",
    "doc_by_vocab = np.empty([len(data), n_feats])\n",
    "\n",
    "def build_vectorizer(max_features, stop_words, max_df=0.8, min_df=10, norm='l2'):\n",
    "    \"\"\"Returns a TfidfVectorizer object with the above preprocessing properties.\n",
    "    \n",
    "    Note: This function may log a deprecation warning. This is normal, and you\n",
    "    can simply ignore it.\n",
    "    \n",
    "    Params: {max_features: Integer,\n",
    "             max_df: Float,\n",
    "             min_df: Float,\n",
    "             norm: String,\n",
    "             stop_words: String}\n",
    "    Returns: TfidfVectorizer\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return TfidfVectorizer(stop_words=stop_words, max_df=max_df, min_df=min_df,max_features=max_features, norm=norm)\n",
    "    \n",
    "tfidf_vec = build_vectorizer(n_feats, \"english\")\n",
    "doc_by_vocab = tfidf_vec.fit_transform([d['text'] for d in data]).toarray()\n",
    "baby_shark_vec = tfidf_vec.fit_transform(baby_shark).toarray()\n",
    "index_to_vocab = {i:v for i, v in enumerate(tfidf_vec.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cos_sim(query,doc):\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

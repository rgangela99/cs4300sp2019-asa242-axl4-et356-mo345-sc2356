{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "#for YouTube video scraping\n",
    "import googleapiclient.discovery\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import unicodedata\n",
    "API_KEY = \"AIzaSyA2l1Gs_fWKE8-UVWhMgVPmF3Bo2-Sci7U\"\n",
    "#for SVD\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#general purpose tokenizer for text input\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "def tokenize(text):\n",
    "    text= text.lower()\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "#building data array of both article text and video description text\n",
    "#to train the vectorizer\n",
    "data = []\n",
    "\n",
    "#dictionaries for referencing the Medium article data set\n",
    "title_to_text={}\n",
    "title_to_index={}\n",
    "link_to_index={}\n",
    "with open('./data/medium/deduped-medium-comments-list.json') as f:\n",
    "    medium_data = json.load(f)\n",
    "i=0\n",
    "for article in medium_data:\n",
    "    title_to_index[article[\"title\"]]=i\n",
    "    title_to_text[article[\"title\"]] = tokenize(article[\"text\"])\n",
    "    link_to_index[article[\"link\"]]=i\n",
    "    tags=\" \"\n",
    "    if \"tags\" in article.keys():\n",
    "        for tag in article[\"tags\"]:\n",
    "            tags=tag+\" \"\n",
    "    data.append(article[\"text\"]+tags)\n",
    "    i+=1\n",
    "\n",
    "#dictionaries for referencing the YouTube videos data set\n",
    "with open('./data/reddit/youtube_comment_data.json') as f:\n",
    "    yt_comment_data = json.load(f)\n",
    "\n",
    "yt_index_to_id={}\n",
    "yt_id_to_text={}\n",
    "yt_id_to_title={}\n",
    "yt_id_to_likes={}\n",
    "yt_id_to_comment={}\n",
    "with open('./data/reddit/youtube_video_data.json') as f:\n",
    "    yt_data = json.load(f)\n",
    "for comment in yt_comment_data:\n",
    "    yt_id_to_comment[comment[\"id\"]] = comment[\"text\"]\n",
    "\n",
    "i=0\n",
    "for youtube in yt_data:\n",
    "    yt_index_to_id[i]=youtube['id']\n",
    "    yt_id_to_text[youtube['id']] = tokenize(youtube[\"snippet\"][\"description\"])\n",
    "    yt_id_to_title[youtube['id']]=youtube[\"snippet\"][\"title\"]\n",
    "    yt_id_to_likes[youtube['id']]=0\n",
    "    if 'statistics' in youtube.keys():\n",
    "        if 'likeCount' in youtube['statistics'].keys():\n",
    "            yt_id_to_likes[youtube['id']]=int(youtube['statistics']['likeCount'])\n",
    "    if youtube['id'] not in yt_id_to_comment.keys():\n",
    "        yt_id_to_comment[youtube['id']]=\"\"\n",
    "    tags=\" \"\n",
    "    if 'tags' in youtube[\"snippet\"].keys():\n",
    "        for tag in youtube[\"snippet\"][\"tags\"]:\n",
    "            tags=tag+\" \"\n",
    "    data.append(youtube[\"snippet\"][\"title\"]+tags)\n",
    "    i+=1\n",
    "\n",
    "#maximum number of features to train the vectorizer\n",
    "n_feats = 5000\n",
    "medium_articles_by_vocab = np.empty([len(medium_data), n_feats])\n",
    "yt_vids_by_vocab = np.empty([len(yt_data), n_feats])\n",
    "# doc_by_vocab = np.empty([len(data), n_feats])\n",
    "\n",
    "def build_vectorizer(max_features, stop_words, max_df=0.8, min_df=10, norm='l2'):\n",
    "    return TfidfVectorizer(stop_words=stop_words, max_df=max_df, min_df=min_df,max_features=max_features, norm=norm)\n",
    "\n",
    "#building vectorizer to train\n",
    "tfidf_vec = build_vectorizer(n_feats, \"english\")\n",
    "tfidf_vec.fit(d for d in data)\n",
    "medium_articles_by_vocab = tfidf_vec.transform(art[\"text\"] for art in medium_data).toarray()\n",
    "yt_vids_by_vocab = tfidf_vec.transform(vid[\"snippet\"][\"description\"] for vid in yt_data).toarray()\n",
    "# doc_by_vocab = tfidf_vec.fit_transform([d['text'] for d in data]).toarray()\n",
    "# tfidf_vec2 = build_vectorizer(n_feats, \"english\")\n",
    "# yt_doc_by_vocab = tfidf_vec2.fit_transform([d[\"snippet\"]['description'] for d in data2]).toarray()\n",
    "index_to_vocab = {i:v for i, v in enumerate(tfidf_vec.get_feature_names())}\n",
    "\n",
    "#returns list of cosine similarities of query vector with every document in provided\n",
    "#tf-idf matrix [doc_by_vocab]\n",
    "def cosine_sim(vec1,doc_by_vocab):\n",
    "    sims = []\n",
    "    i=0\n",
    "    for doc in doc_by_vocab:\n",
    "        if(np.linalg.norm(vec1)*np.linalg.norm(doc))==0:\n",
    "            sims.append(0)\n",
    "        else:\n",
    "            sims.append(np.dot(vec1,doc)/(np.linalg.norm(vec1)*np.linalg.norm(doc)))\n",
    "    return sims\n",
    "\n",
    "\n",
    "\n",
    "def SVD(tf_idf_matrix):\n",
    "    svd_matrix = (tf_idf_matrix).transpose()\n",
    "    words_compressed, _, docs_compressed = svds(svd_matrix, k=100)\n",
    "    docs_compressed = docs_compressed.transpose()\n",
    "    #words_compressed = normalize(words_compressed, axis = 1)\n",
    "    docs_compressed = normalize(docs_compressed, axis = 1)\n",
    "    return docs_compressed\n",
    "\n",
    "\n",
    "#YouTube video scraping\n",
    "def url_to_id(url):\n",
    "    if '?v=' in url:\n",
    "        vid_id = url.split('?v=')[1]\n",
    "        and_idx = vid_id.find('&')\n",
    "\n",
    "        if and_idx != -1:\n",
    "            vid_id = vid_id[:and_idx]\n",
    "\n",
    "        return vid_id\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_video_info(vids):\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "#     os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    DEVELOPER_KEY = API_KEY\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey = DEVELOPER_KEY)\n",
    "\n",
    "    id_string = \"\"\n",
    "\n",
    "    for i in range(len(vids) - 1):\n",
    "        id_string += vids[i] + \",\"\n",
    "\n",
    "    id_string += vids[-1]\n",
    "\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=id_string\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_single_video(vid_id):\n",
    "    return get_video_info([vid_id])\n",
    "\n",
    "def vid_url_to_title(vid_url):\n",
    "    return get_single_video(url_to_id(vid_url))['items'][0]['snippet']['title']\n",
    "\n",
    "def art_url_to_title(art_url):\n",
    "    data = requests.get(art_url)\n",
    "    soup = BeautifulSoup(data.content, 'html.parser')\n",
    "    title = soup.findAll('title')[0]\n",
    "    title = title.get_text()\n",
    "    return title\n",
    "\n",
    "def claps_to_nums(claps):\n",
    "    if claps == 0:\n",
    "        return 0\n",
    "    num=claps.split()[0]\n",
    "    if \"K\" in num:\n",
    "        num=num[:-1]\n",
    "        num=float(num)*1000\n",
    "    else:\n",
    "        num=float(num)\n",
    "    return num\n",
    "\n",
    "#search function from YouTube video to Medium article\n",
    "def mediumSearch(query, time_range):\n",
    "    vid_id = url_to_id(query)\n",
    "    api_response = get_single_video(vid_id)\n",
    "    my_video_info = api_response['items'][0]\n",
    "    my_title = my_video_info['snippet']['title']\n",
    "    query_vec = tfidf_vec.transform([my_title]).toarray()\n",
    "    #demonstrating video description\n",
    "    # vid_desc = my_video_info['snippet']['description']\n",
    "    # query_vec = tfidf_vec.transform([vid_desc]).toarray()\n",
    "   # sims = np.array(cosine_sim(query_vec,medium_articles_by_vocab)).flatten()\n",
    "    return_arr = []\n",
    "    #sort_idx = np.flip(np.argsort(sims))\n",
    "    \n",
    "    \n",
    "    \n",
    "    mat_and_q = np.append(medium_articles_by_vocab,query_vec,axis=0)\n",
    "    svd_docs= SVD(mat_and_q)\n",
    "    sims = np.array(cosine_sim(svd_docs[-1],svd_docs[:-1])).flatten()\n",
    "    sort_idx = np.flip(np.argsort(sims))\n",
    "\n",
    "    num_returned = 0\n",
    "    while num_returned < 14:\n",
    "        article = medium_data[sort_idx[i]]\n",
    "        if article[\"reading_time\"] >= time_range[0] and article[\"reading_time\"] <= time_range[1]:\n",
    "            num_returned += 1\n",
    "            return_arr.append((article[\"title\"], article[\"link\"], article[\"comments\"][0] if len(article[\"comments\"])>0 else \"\",int(claps_to_nums(article[\"claps\"]))))\n",
    "\n",
    "    clap_arr = []\n",
    "    for j in range(0,15):\n",
    "        art_index = title_to_index[return_arr[j][0]]\n",
    "        claps=medium_data[art_index][\"claps\"]\n",
    "        claps_to_nums(claps)\n",
    "        clap_arr.append(claps_to_nums(claps))\n",
    "\n",
    "    clap_return_arr=[]\n",
    "    for k in range(0,15):\n",
    "        clap_return_arr.append(return_arr[np.argmax(clap_arr)])\n",
    "        clap_arr[np.argmax(clap_arr)]=0\n",
    "\n",
    "    return clap_return_arr\n",
    "\n",
    "#search function from Medium article to YouTube video\n",
    "def youtubeSearch(query):\n",
    "    try:\n",
    "        data = requests.get(query)\n",
    "        soup = BeautifulSoup(data.content, 'html.parser')\n",
    "        paras = soup.findAll('p')\n",
    "        text = ''\n",
    "        nxt_line = '\\n'\n",
    "        for para in paras:\n",
    "            text += unicodedata.normalize('NFKD',\n",
    "                                            para.get_text()) + nxt_line\n",
    "        query_vec = tfidf_vec.transform([text]).toarray()\n",
    "        #sims = np.array(cosine_sim(query_vec,yt_vids_by_vocab)).flatten()\n",
    "        mat_and_q = np.append(yt_vids_by_vocab,query_vec,axis=0)\n",
    "        svd_docs= SVD(mat_and_q)\n",
    "        sims = np.array(cosine_sim(svd_docs[-1],svd_docs[:-1])).flatten()\n",
    "        return_arr= []\n",
    "        sort_idx =  np.flip(np.argsort(sims))\n",
    "        id_arr = []\n",
    "\n",
    "        for i in range(0,15):\n",
    "            curr_id = yt_index_to_id[sort_idx[i]]\n",
    "            return_arr.append((yt_id_to_title[curr_id],\"https://www.youtube.com/watch?v=\"+curr_id, yt_id_to_comment[curr_id], yt_id_to_likes[curr_id]))\n",
    "            id_arr.append(curr_id)\n",
    "\n",
    "        like_arr = [yt_id_to_likes[i] for i in id_arr]\n",
    "        like_return_arr=[]\n",
    "        for k in range(0,15):\n",
    "            like_return_arr.append(return_arr[np.argmax(like_arr)])\n",
    "            like_arr[np.argmax(like_arr)]=0\n",
    "\n",
    "        return like_return_arr\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return [(\"This is not a recognized Medium article link\",\"\")]\n",
    "\n",
    "def getLink(query):\n",
    "    if(query == \"\"):\n",
    "        return 0\n",
    "    else:\n",
    "        return mediumSearch(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78oMjNCAayQ\n"
     ]
    }
   ],
   "source": [
    "with open('./data/reddit/youtube_video_data.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(data[0]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_lengths = {}\n",
    "regex = re.compile(r'PT(?:(?:(?P<hr>\\d+)H)?(?:(?P<min>\\d+)M)?(?:(?P<sec>\\d+)S)?)')\n",
    "for entry in data:\n",
    "    if 'contentDetails' not in entry:\n",
    "        continue\n",
    "    match = regex.fullmatch(entry['contentDetails']['duration'])\n",
    "    length = 0\n",
    "    hrs = match.group('hr')\n",
    "    mins = match.group('min')\n",
    "    secs = match.group('sec')\n",
    "\n",
    "    if hrs:\n",
    "        length += int(hrs) * 60\n",
    "    if mins:\n",
    "        length += int(mins)\n",
    "    if secs:\n",
    "        length += int(secs) / 60\n",
    "\n",
    "    video_lengths[entry['id']] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374\n",
      "1367\n",
      "30.25107290904658\n"
     ]
    }
   ],
   "source": [
    "avg_length = 0\n",
    "num_entries = 0\n",
    "for entry in data:\n",
    "    if entry['id'] in video_lengths:\n",
    "        avg_length += video_lengths[entry['id']]\n",
    "        num_entries += 1\n",
    "print(len(data))\n",
    "print(num_entries)\n",
    "print(avg_length / num_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/reddit/youtube_video_lengths.pickle', 'wb') as f:\n",
    "    pickle.dump(video_lengths, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/anjelikalynne/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "#for YouTube video scraping\n",
    "import googleapiclient.discovery\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import unicodedata\n",
    "API_KEY = \"AIzaSyA2l1Gs_fWKE8-UVWhMgVPmF3Bo2-Sci7U\"\n",
    "#for SVD\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "#for Sentiment Analysis\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "#general purpose tokenizer for text input\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "def tokenize(text):\n",
    "    text= text.lower()\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "\n",
    "def claps_to_nums(claps):\n",
    "    if claps == 0:\n",
    "        return 0\n",
    "    num=claps.split()[0]\n",
    "    if \"K\" in num:\n",
    "        num=num[:-1]\n",
    "        num=float(num)*1000\n",
    "    else:\n",
    "        num=float(num)\n",
    "    return num\n",
    "\n",
    "#building data arrays for Medium article text and YouTube video plus tags\n",
    "#for those that have tags\n",
    "med_text_tag = []\n",
    "yt_title_tag = []\n",
    "\n",
    "#dictionary for referencing the Medium article data set\n",
    "medium_ind_to_art_info = {}\n",
    "\n",
    "with open('./data/medium/deduped-medium-comments-list.json') as f:\n",
    "    medium_data = json.load(f)\n",
    "\n",
    "i=0\n",
    "for article in medium_data:\n",
    "    tmp = {}\n",
    "    tmp[\"title\"] = article[\"title\"]\n",
    "    tmp[\"link\"] = article[\"link\"]\n",
    "    tmp[\"claps\"] = int(claps_to_nums(article[\"claps\"]))\n",
    "    tmp[\"reading_time\"] = article[\"reading_time\"]\n",
    "    if (len(article[\"comments\"])>0):\n",
    "        tmp[\"comments\"] = article[\"comments\"]\n",
    "        comment_toks = set()\n",
    "        sentiments=[]\n",
    "        for comment in article[\"comments\"]:\n",
    "            sentiments.append(sid.polarity_scores((comment).lower()))\n",
    "            comment_toks.update(tokenize(comment))\n",
    "        tmp[\"sentiments\"] = sentiments\n",
    "        tmp[\"comment_toks\"] = comment_toks\n",
    "    art_text_tag = article[\"text\"]\n",
    "    if \"tags\" in article.keys():\n",
    "        tags=set()\n",
    "        for tag in article[\"tags\"]:\n",
    "            art_text_tag += \" \" + tag\n",
    "            tags.add(tag)\n",
    "        tmp[\"tags\"] = tags\n",
    "    med_text_tag.append(art_text_tag)\n",
    "    medium_ind_to_art_info[i] = tmp\n",
    "    i+=1 \n",
    "\n",
    "med_data_len = len(medium_ind_to_art_info.keys())\n",
    "\n",
    "with open('./data/reddit/youtube_comment_data.json') as f:\n",
    "    yt_comment_data = json.load(f)\n",
    "\n",
    "with open('./data/reddit/youtube_video_lengths.pickle', 'rb') as f:\n",
    "    yt_id_to_length = pickle.load(f)\n",
    "\n",
    "#dictionaries for referencing the YouTube videos data set\n",
    "yt_index_to_id = {}\n",
    "yt_id_to_vid_info = {}\n",
    "with open('./data/reddit/youtube_video_data.json') as f:\n",
    "    yt_data = json.load(f)\n",
    "\n",
    "i=0\n",
    "for youtube in yt_data:\n",
    "    yt_id=youtube['id']\n",
    "    yt_index_to_id[i]=yt_id\n",
    "    yt_id_to_vid_info[yt_id]={}\n",
    "    yt_id_to_vid_info[yt_id][\"title\"] = youtube[\"snippet\"][\"title\"]\n",
    "    yt_id_to_vid_info[yt_id][\"likes\"] = 0\n",
    "    if 'statistics' in youtube.keys():\n",
    "        if 'likeCount' in youtube['statistics'].keys():\n",
    "            yt_id_to_vid_info[yt_id][\"likes\"] = int(youtube['statistics']['likeCount'])\n",
    "    vid_title_tag = youtube[\"snippet\"][\"title\"]\n",
    "    if 'tags' in youtube[\"snippet\"].keys():\n",
    "        #tags=\" \"\n",
    "        tags = set()\n",
    "        for tag in youtube[\"snippet\"][\"tags\"]:\n",
    "            vid_title_tag += \" \" + tag\n",
    "            tags.add(tag)\n",
    "        yt_id_to_vid_info[yt_id][\"tags\"] = tags\n",
    "    yt_title_tag.append(vid_title_tag)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "for vid_comments in yt_comment_data:\n",
    "    top_comments = []\n",
    "    comment_toks = set()\n",
    "    sentiments=[]\n",
    "    #comment[0] is the actual text of the comment\n",
    "    #comment[1] is the number of likes for that comment\n",
    "    for comment in vid_comments[\"text_likes\"]:\n",
    "        top_comments.append(comment[0])\n",
    "        sentiments.append(sid.polarity_scores((comment[0]).lower()))\n",
    "        comment_toks.update(tokenize(comment[0]))\n",
    "    yt_id = vid_comments[\"id\"]\n",
    "    yt_id_to_vid_info[yt_id][\"comments\"] = top_comments\n",
    "    yt_id_to_vid_info[yt_id][\"comment_toks\"] = comment_toks\n",
    "    yt_id_to_vid_info[yt_id][\"sentiments\"] = sentiments\n",
    "\n",
    "yt_data_len = len(yt_index_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/medium/medium-data.pickle', 'wb') as f:\n",
    "    pickle.dump(medium_ind_to_art_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/reddit/youtube-vid-info.pickle', 'wb') as f:\n",
    "    pickle.dump(yt_id_to_vid_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/reddit/youtube-index-id.pickle', 'wb') as f:\n",
    "    pickle.dump(yt_index_to_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data array of both article text and video description text\n",
    "#to train the vectorizer\n",
    "data = med_text_tag + yt_title_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum number of features to train the vectorizer\n",
    "n_feats = 5000\n",
    "medium_articles_by_vocab = np.empty([med_data_len, n_feats])\n",
    "yt_vids_by_vocab = np.empty([yt_data_len, n_feats])\n",
    "# doc_by_vocab = np.empty([len(data), n_feats])\n",
    "\n",
    "def build_vectorizer(max_features, stop_words, max_df=0.8, min_df=10, norm='l2'):\n",
    "    return TfidfVectorizer(stop_words=stop_words, max_df=max_df, min_df=min_df,max_features=max_features, norm=norm)\n",
    "\n",
    "#building vectorizer to train\n",
    "tfidf_vec = build_vectorizer(n_feats, \"english\")\n",
    "tfidf_vec.fit(d for d in data)\n",
    "medium_articles_by_vocab = tfidf_vec.transform(art for art in med_text_tag).toarray()\n",
    "yt_vids_by_vocab = tfidf_vec.transform(vid for vid in yt_title_tag).toarray()\n",
    "# doc_by_vocab = tfidf_vec.fit_transform([d['text'] for d in data]).toarray()\n",
    "# tfidf_vec2 = build_vectorizer(n_feats, \"english\")\n",
    "# yt_doc_by_vocab = tfidf_vec2.fit_transform([d[\"snippet\"]['description'] for d in data2]).toarray()\n",
    "index_to_vocab = {i:v for i, v in enumerate(tfidf_vec.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/medium/medium-matrix.pickle', 'wb') as f:\n",
    "    pickle.dump(medium_articles_by_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/reddit/youtube-matrix.pickle', 'wb') as f:\n",
    "    pickle.dump(yt_vids_by_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf_vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVD(k_val):\n",
    "    return TruncatedSVD(n_components=k_val)\n",
    "\n",
    "med_k_val = 100\n",
    "yt_k_val = 200\n",
    "#train different SVD models on different spaces depending on the data set\n",
    "svd_med = SVD(med_k_val)\n",
    "svd_yt = SVD(yt_k_val)\n",
    "svd_med_docs = svd_med.fit_transform(medium_articles_by_vocab)\n",
    "svd_yt_docs = svd_yt.fit_transform(yt_vids_by_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/SVD-med-model.pickle', 'wb') as f:\n",
    "    pickle.dump(svd_med, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/SVD-yt-model.pickle', 'wb') as f:\n",
    "    pickle.dump(svd_yt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/SVD-med-docs.pickle', 'wb') as f:\n",
    "    pickle.dump(svd_med_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/SVD-yt-docs.pickle', 'wb') as f:\n",
    "    pickle.dump(svd_yt_docs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
